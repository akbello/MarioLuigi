{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d99bee",
   "metadata": {},
   "source": [
    "# MarioLuigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff813b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734782127.256234  250775 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1734782127.265081  251131 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1734782127.271560  251131 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1734782129.086894  251137 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Mediapipe Hand model with detection and tracking confidence\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# To track whether a key is currently being held down\n",
    "current_key = None\n",
    "last_press_time = time.time()  # Track the last press time\n",
    "press_delay = 0.2  # Delay between key presses (in seconds)\n",
    "\n",
    "# Counters for each gesture (click count)\n",
    "gesture_counts = {\n",
    "    'up': 0,\n",
    "    'down': 0,\n",
    "    'left': 0,\n",
    "    'right': 0\n",
    "}\n",
    "\n",
    "# Define font and text size\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.9\n",
    "font_thickness = 2\n",
    "\n",
    "# Load logo\n",
    "overlay_img = cv2.imread(\"llama.jpg\", cv2.IMREAD_UNCHANGED)\n",
    "overlay_img = cv2.resize(overlay_img, (50, 50)) \n",
    "\n",
    "# Function to detect specific gestures based on hand landmarks\n",
    "def detect_gesture(hand_landmarks, frame_width, is_mirror=False):\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    index_tip = hand_landmarks.landmark[8]\n",
    "    middle_tip = hand_landmarks.landmark[12]\n",
    "    ring_tip = hand_landmarks.landmark[16]\n",
    "    pinky_tip = hand_landmarks.landmark[20]\n",
    "    wrist = hand_landmarks.landmark[0]\n",
    "\n",
    "    if is_mirror:\n",
    "        thumb_tip.x = 1 - thumb_tip.x\n",
    "        index_tip.x = 1 - index_tip.x\n",
    "        middle_tip.x = 1 - middle_tip.x\n",
    "        ring_tip.x = 1 - ring_tip.x\n",
    "        pinky_tip.x = 1 - pinky_tip.x\n",
    "        wrist.x = 1 - wrist.x\n",
    "\n",
    "    # Gesture: Up (all fingers extended and well spread)\n",
    "    if (all([\n",
    "        index_tip.y < wrist.y,  # Above wrist\n",
    "        middle_tip.y < wrist.y,\n",
    "        ring_tip.y < wrist.y,\n",
    "        pinky_tip.y < wrist.y,\n",
    "    ]) and abs(thumb_tip.x - pinky_tip.x) > 0.3):\n",
    "        return 'up'\n",
    "\n",
    "    # Gesture: Down (fist gesture, all fingers curled)\n",
    "    if (\n",
    "        all([index_tip.y > wrist.y, middle_tip.y > wrist.y, ring_tip.y > wrist.y, pinky_tip.y > wrist.y])\n",
    "        and abs(index_tip.x - thumb_tip.x) < 0.1\n",
    "    ):\n",
    "        return 'down'\n",
    "\n",
    "    # Gesture: Left (index and thumb extended, others curled)\n",
    "    if (\n",
    "        index_tip.y < wrist.y\n",
    "        and middle_tip.y > index_tip.y\n",
    "        and abs(index_tip.x - thumb_tip.x) > 0.1\n",
    "        and (index_tip.x < thumb_tip.x if not is_mirror else index_tip.x > thumb_tip.x)\n",
    "    ):\n",
    "        return 'left'\n",
    "\n",
    "    # Gesture: Right (index and thumb extended, others curled)\n",
    "    if (\n",
    "        index_tip.y < wrist.y\n",
    "        and middle_tip.y > index_tip.y\n",
    "        and abs(index_tip.x - thumb_tip.x) > 0.1\n",
    "        and (index_tip.x > thumb_tip.x if not is_mirror else index_tip.x < thumb_tip.x)\n",
    "    ):\n",
    "        return 'right'\n",
    "\n",
    "    return None\n",
    "\n",
    "# Function to simulate a key click (press and release the key)\n",
    "def click_key(direction):\n",
    "    global current_key, last_press_time\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - last_press_time >= press_delay:\n",
    "        if direction == 'up':\n",
    "            if current_key != 'up':\n",
    "                pyautogui.press('up')\n",
    "                current_key = 'up'\n",
    "                gesture_counts['up'] += 1\n",
    "        elif direction == 'down':\n",
    "            if current_key != 'down':\n",
    "                pyautogui.press('down')\n",
    "                current_key = 'down'\n",
    "                gesture_counts['down'] += 1\n",
    "        elif direction == 'left':\n",
    "            if current_key != 'left':\n",
    "                pyautogui.press('left')\n",
    "                current_key = 'left'\n",
    "                gesture_counts['left'] += 1\n",
    "        elif direction == 'right':\n",
    "            if current_key != 'right':\n",
    "                pyautogui.press('right')\n",
    "                current_key = 'right'\n",
    "                gesture_counts['right'] += 1\n",
    "        last_press_time = current_time\n",
    "\n",
    "# Function to press and hold the \"down\" key\n",
    "def press_key(direction):\n",
    "    global current_key, last_press_time\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - last_press_time >= press_delay:\n",
    "        if direction == 'down':\n",
    "            if current_key != 'down':\n",
    "                pyautogui.keyDown('down')\n",
    "                current_key = 'down'\n",
    "                gesture_counts['down'] += 1\n",
    "        last_press_time = current_time\n",
    "\n",
    "# Function to release the key\n",
    "def release_key():\n",
    "    global current_key\n",
    "    if current_key:\n",
    "        pyautogui.keyUp(current_key)\n",
    "        current_key = None\n",
    "\n",
    "# Function to draw gesture counts on the frame\n",
    "def draw_background(frame, text_position, color, transparency=0.6):\n",
    "    x, y = text_position\n",
    "    text = f\"Up: {gesture_counts['up']} | Down: {gesture_counts['down']} | Left: {gesture_counts['left']} | Right: {gesture_counts['right']}\"\n",
    "    (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "\n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (x - 10, y - 50), (x + text_width + 10, y + text_height + 10), color, -1)\n",
    "    cv2.addWeighted(overlay, transparency, frame, 1 - transparency, 0, frame)\n",
    "\n",
    "    cv2.putText(frame, text, (x, y), font, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)\n",
    "\n",
    "# Function to draw performance percentage\n",
    "def draw_performance(frame):\n",
    "    total_gestures = sum(gesture_counts.values())\n",
    "    if total_gestures == 0:\n",
    "        performance_percentage = 0\n",
    "    else:\n",
    "        performance_percentage = (total_gestures / (4 * max(gesture_counts.values(), default=1))) * 100\n",
    "\n",
    "    performance_text = f\"Performance: {performance_percentage:.2f}%\"\n",
    "    cv2.putText(frame, performance_text, (10, frame.shape[0] - 10), font, 0.9, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "# Main loop to process video frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to grab frame from webcam.\")\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame = cv2.resize(frame, (600, 400))\n",
    "\n",
    "    # Draw a thin border around the frame\n",
    "    border_thickness = 50\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    border_color = (201, 170, 136)  # Border colour\n",
    "\n",
    "    # Draw the border (top-left and bottom-right corners)\n",
    "    cv2.rectangle(frame, (0, 0), (frame_width-1, frame_height-1), border_color, border_thickness)\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    detected_gesture = None\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            detected_gesture = detect_gesture(hand_landmarks, frame_width=frame.shape[1], is_mirror=True)\n",
    "\n",
    "    if detected_gesture:\n",
    "        if detected_gesture == 'down':\n",
    "            press_key(detected_gesture)\n",
    "        else:\n",
    "            click_key(detected_gesture)\n",
    "    else:\n",
    "        release_key()\n",
    "\n",
    "    draw_background(frame, (10, 50), (0, 0, 0))\n",
    "    draw_performance(frame)\n",
    "    \n",
    "    # Logo on the bottom-right corner\n",
    "    h, w, _ = overlay_img.shape  # Dimensions of the overlay image\n",
    "    x_offset = frame.shape[1] - w - 10  # Bottom-right corner with 10px margin\n",
    "    y_offset = frame.shape[0] - h - 10\n",
    "\n",
    "    if overlay_img.shape[2] == 4:\n",
    "        bgr_img = overlay_img[:, :, :3]\n",
    "        alpha_channel = overlay_img[:, :, 3] / 255.0\n",
    "\n",
    "        for c in range(0, 3):  \n",
    "            frame[y_offset:y_offset+h, x_offset:x_offset+w, c] = \\\n",
    "                frame[y_offset:y_offset+h, x_offset:x_offset+w, c] * (1 - alpha_channel) + \\\n",
    "                bgr_img[:, :, c] * alpha_channel\n",
    "    else:\n",
    "        frame[y_offset:y_offset+h, x_offset:x_offset+w] = overlay_img\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Hand Gesture Control\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c47f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
